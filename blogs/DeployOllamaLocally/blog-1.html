<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deploy Ollama Locally - Jibby George</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 1.5rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        header img {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            object-fit: cover;
            border: 4px solid white;
        }
        
        header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            color: #800000;
        }
        
        header p {
            font-size: 1.1rem;
            opacity: 0.9;
            color: white;
        }
        
        nav {
            background: #f8f9fa;
            padding: 1rem 0;
            position: sticky;
            top: 0;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        }
        
        nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
            max-width: 1000px;
            margin: 0 auto;
            padding: 0 2rem;
        }
        
        nav a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        
        nav a:hover {
            color: #764ba2;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
        }
        
        .back-btn {
            display: inline-block;
            margin-bottom: 2rem;
            padding: 0.75rem 1.5rem;
            background: #800000;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 600;
            transition: background 0.3s ease, transform 0.2s ease;
        }
        
        .back-btn:hover {
            background: #600000;
            transform: translateY(-2px);
        }
        
        .blog-content {
            background: white;
            padding: 2.5rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        .blog-content h1 {
            color: #800000;
            margin-bottom: 0.5rem;
            font-size: 2.5rem;
        }
        
        .blog-meta {
            color: #666;
            margin-bottom: 2rem;
            font-size: 0.95rem;
        }
        
        .blog-content h2 {
            color: #667eea;
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-size: 1.8rem;
        }
        
        .blog-content h3 {
            color: #667eea;
            margin-top: 1.5rem;
            margin-bottom: 0.8rem;
            font-size: 1.3rem;
        }
        
        .blog-content p {
            margin-bottom: 1rem;
            line-height: 1.8;
        }
        
        .blog-content ul, .blog-content ol {
            margin-left: 2rem;
            margin-bottom: 1rem;
        }
        
        .blog-content li {
            margin-bottom: 0.5rem;
        }
        
        .code-block {
            background: #f5f5f5;
            border-left: 4px solid #800000;
            padding: 1rem;
            margin: 1.5rem 0;
            border-radius: 4px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
        }
        
        .highlight {
            background: #fff3cd;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
        }
        
        .blog-image {
            max-width: 100%;
            height: auto;
            margin: 2rem 0;
            border-radius: 6px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        footer {
            background: #2c3e50;
            color: white;
            text-align: center;
            padding: 2rem;
            margin-top: 3rem;
        }
    </style>
</head>
<body>
    <header>
        <img src="/assets/images/profile.jpg" alt="Profile Picture">
        <div class="header-content">
            <h1>Jibby George</h1>
            <p>Built with HTML, CSS & hosted on GitHub Pages</p>
        </div>
    </header>
    
    <nav>
        <ul>
            <li><a href="/">Home</a></li>
            <li><a href="/#about">About</a></li>
            <li><a href="/#projects">Projects</a></li>
            <li><a href="/#blogs">Blogs</a></li>
            <li><a href="/#contact">Contact</a></li>
        </ul>
    </nav>
    
    <div class="container">
        <a href="/#blogs" class="back-btn">‚Üê Back to Blogs</a>
        
        <div class="blog-content">
            <h1>Deploy Ollama Locally</h1>
            <p class="blog-meta">Published on January 5, 2025</p>
            
            <h2>What is Ollama</h2>
            <p>Ollama is a way to run Open Source LLM (Large Language Models) on your local machine. It provides a simple and efficient way to interact with various open-source language models without relying on cloud services.</p>
            
            <h2>Interacting with OSS LLM using Ollama</h2>
            <p>To get started with Ollama, you need to install it from the official download link. I have installed Ollama in both Windows and WSL (Windows Subsystem for Linux) environments.</p>
            
            <h3>Ollama on Windows Terminal</h3>
            <p>Once installed, you can interact with Ollama directly from the Windows Terminal. The installation provides a command-line interface to manage and run your language models.</p>
            
            <h3>Update Ollama on WSL</h3>
            <p>For WSL users, you can update Ollama using your package manager to ensure you have the latest version with all the latest features and improvements.</p>
            
            <h3>Ollama GUI on Windows</h3>
            <p>Ollama also provides a graphical user interface on Windows for easier interaction. You need to register an account with Ollama to use cloud models. Below, I'm using the deepseek-v3 model which is running in the cloud.</p>
            
            <h2>Pulling Images from Model Repository</h2>
            <p>You can pull images from the model repository and run them locally on your machine.</p>
            <p><strong>Ollama pull</strong> will pull the image from the repository so you can use it offline:</p>
            <div class="code-block">
ollama pull llama2
            </div>
            
            <h3>Deleting an Image and Freeing Space</h3>
            <p>To remove an image and free up disk space, you can use the delete command:</p>
            <div class="code-block">
ollama rm llama2
            </div>
            
            <h2>Connecting Ollama with REST API</h2>
            <p><span class="highlight">Ollama exposes a REST API that you can use to integrate it with your applications.</span></p>
            <ul>
                <li><strong>Local port:</strong> 11434</li>
                <li><strong>Endpoints:</strong> /api/generate and /api/chat</li>
            </ul>
            <p>Example curl command to interact with the API:</p>
            <div class="code-block">
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2",
    "prompt": "Why is the capital of Australia?"
  }'
            </div>
            
            <h2>Connecting Ollama with Python</h2>
            <p>You can easily integrate Ollama with Python using the official client library:</p>
            <div class="code-block">
pip install ollama
import ollama

# Use the generate function
result = ollama.generate(model='llama2', prompt='Why is the capital of Australia?')
print(result['response'])

# Use Chat API
from ollama import chat
conversation = [
    {"role": "user", "content": "Hello, how are you?"}
]
reply = chat(model='llama2', messages=conversation)
print(reply.message.content)
            </div>
            
            <h2>Connecting Ollama with OpenAI Compatible Endpoint</h2>
            <p>Ollama provides an OpenAI-compatible endpoint, allowing you to use it as a drop-in replacement for OpenAI's API. This makes it easy to integrate with existing applications that use the OpenAI SDK.</p>
            <p>You can configure your application to use <code>http://localhost:11434/v1</code> as the base URL and any model that you have pulled locally.</p>
        </div>
    </div>
    
    <footer>
        <p>&copy; 2026 Jibby George. All rights reserved.</p>
    </footer>
</body>
</html>